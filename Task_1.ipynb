{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "328347d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/klinhfhm/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/klinhfhm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "05262450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "985cf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english') \n",
    "    \n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and \n",
    "            word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "    return tweets_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0a081abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    freqs = defaultdict(int)\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            freqs[(word, y[0])] += 1\n",
    "    return freqs\n",
    "\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "train_x = all_positive_tweets[:4000] + all_negative_tweets[:4000]\n",
    "train_y = np.append(np.ones((4000,1)), np.zeros((4000,1)), axis=0)\n",
    "\n",
    "freqs = build_freqs(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd9facc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(tweet, freqs):\n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros((1, 3)) \n",
    "    x[0,0] = 1 \n",
    "    for word in word_l:\n",
    "        x[0,1] += freqs.get((word,1.0), 0)\n",
    "        x[0,2] += freqs.get((word,0.0), 0)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b1610daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bdb6cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logistic(X, y, w, alpha, num_iters=100):\n",
    "    m = X.shape[0]\n",
    "    for i in range(num_iters):\n",
    "        z = np.dot(X, w)\n",
    "        h = sigmoid(z)\n",
    "        J = -1/m * (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n",
    "        w -= alpha/m * np.dot(X.T, (h-y))\n",
    "    return J, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4c911b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic(X, w):\n",
    "    z = np.dot(X, w)\n",
    "    h = sigmoid(z)\n",
    "    return (h >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "73ed777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e501f5",
   "metadata": {},
   "source": [
    "**TASK 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4409642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Scaler: Precision = 0.9845\n",
      "MinMax: Precision = 0.8972\n",
      "Standard: Precision = 0.9796\n",
      "Robust: Precision = 0.9773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "scalers = {\n",
    "    \"No Scaler\": None,\n",
    "    \"MinMax\": MinMaxScaler(),\n",
    "    \"Standard\": StandardScaler(),\n",
    "    \"Robust\": RobustScaler()\n",
    "}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    if scaler:\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = X\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_scaled, Y.ravel())\n",
    "    y_pred = clf.predict(X_scaled)\n",
    "    precision = precision_score(Y, y_pred)\n",
    "    print(f\"{name}: Precision = {precision:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "91bad471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Scaler: Precision = 0.9926\n",
      "MinMax: Precision = 0.9019\n",
      "Standard: Precision = 0.9864\n",
      "Robust: Precision = 0.9816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Giả sử X, Y đã có sẵn và Y là 1D\n",
    "# Nếu Y đang ở dạng (n,1): ép về 1D\n",
    "Y = Y.ravel()\n",
    "\n",
    "# Loại bỏ cột bias (nếu có). Giả sử bias là cột 0:\n",
    "X_feats = X[:, 1:]   # giữ chỉ các đặc trưng thực (pos_count, neg_count, ...)\n",
    "\n",
    "# Tạo train/test split (stratify để giữ cân bằng nhãn)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_feats, Y, test_size=0.2, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "scalers = {\n",
    "    \"No Scaler\": None,\n",
    "    \"MinMax\": MinMaxScaler(),\n",
    "    \"Standard\": StandardScaler(),\n",
    "    \"Robust\": RobustScaler()\n",
    "}\n",
    "results = {}\n",
    "for name, scaler in scalers.items():\n",
    "    if scaler is None:\n",
    "        X_train_s = X_train.copy()\n",
    "        X_test_s  = X_test.copy()\n",
    "    else:\n",
    "        # Fit only on train to avoid leakage\n",
    "        scaler.fit(X_train)\n",
    "        X_train_s = scaler.transform(X_train)\n",
    "        X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "    # Train model (Logistic Regression)\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    clf.fit(X_train_s, y_train)\n",
    "    y_pred = clf.predict(X_test_s)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    results[name] = precision\n",
    "    print(f\"{name}: Precision = {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b98a7",
   "metadata": {},
   "source": [
    "**No Scaler:** Precision = 0.9845\n",
    "→ This is the highest score, indicating that raw word frequency features are already on a stable and comparable scale. Logistic Regression can directly exploit them without normalization.\n",
    "\n",
    "**Min-Max Scaler:** Precision = 0.8972\n",
    "→ Precision drops significantly. By compressing all feature values into the [0,1] range, MinMax scaling reduces the contrast between tweets with many positive/negative words and those with only a few. As a result, the decision boundary becomes less discriminative.\n",
    "\n",
    "**Standard Scaler:** Precision = 0.9796\n",
    "→ Performance is close to the unscaled baseline. Even though word counts are not normally distributed, standardization preserves the relative proportions between positive and negative frequencies.\n",
    "\n",
    "**Robust Scaler:** Precision = 0.9773\n",
    "→ Also close to the baseline. Since the dataset does not contain extreme outliers in word counts, the median/IQR scaling does not bring additional benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84699f",
   "metadata": {},
   "source": [
    "**TASK 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893b590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall        F1\n",
      "0         Bernoulli NB  0.508750   1.000000  0.022388  0.043796\n",
      "1       Multinomial NB  0.993125   0.990111  0.996269  0.993180\n",
      "2        Random Forest  0.993125   0.990111  0.996269  0.993180\n",
      "3                  KNN  0.992500   0.988889  0.996269  0.992565\n",
      "4       MLP Neural Net  0.987500   0.976886  0.998756  0.987700\n",
      "5           Linear SVM  0.985000   0.972155  0.998756  0.985276\n",
      "6  Logistic Regression  0.983125   0.968637  0.998756  0.983466\n",
      "\n",
      "Best model: Bernoulli NB → Precision = 1.0\n"
     ]
    }
   ],
   "source": [
    "#cách sử dụng cả các metrics khác để đối chiếu\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Multinomial NB\": MultinomialNB(),\n",
    "    \"Bernoulli NB\": BernoulliNB(),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"MLP Neural Net\": MLPClassifier(hidden_layer_sizes=(32,), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in [\"Logistic Regression\", \"Linear SVM\", \"KNN\", \"MLP Neural Net\"]:\n",
    "        clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=False)), \n",
    "            (\"model\", model)\n",
    "        ])\n",
    "    else:\n",
    "        clf = model\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"binary\")\n",
    "    rec = recall_score(y_test, y_pred, average=\"binary\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "    results.append([name, acc, prec, rec, f1])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "df_results = df_results.sort_values(by=\"Precision\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_results)\n",
    "print(\"\\nBest model:\", df_results.iloc[0][\"Model\"], \"→ Precision =\", df_results.iloc[0][\"Precision\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039cf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model  Precision\n",
      "0          Bernoulli NB   1.000000\n",
      "1        Multinomial NB   0.997503\n",
      "2            Linear SVM   0.997503\n",
      "3   MLP Neural Net (50)   0.997503\n",
      "4               KNN (5)   0.997500\n",
      "5              KNN (10)   0.997500\n",
      "6   Random Forest (100)   0.997497\n",
      "7   Random Forest (200)   0.997497\n",
      "8  MLP Neural Net (100)   0.996259\n",
      "9   Logistic Regression   0.992556\n",
      "\n",
      "Best model: Bernoulli NB với Precision = 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Split fixed train/test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y.ravel(), test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# Define models with basic configuration\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Multinomial NB\": MultinomialNB(),\n",
    "    \"Bernoulli NB\": BernoulliNB(),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000),\n",
    "    \"Random Forest (100)\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Random Forest (200)\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"KNN (5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"KNN (10)\": KNeighborsClassifier(n_neighbors=10),\n",
    "    \"MLP Neural Net (50)\": MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42),\n",
    "    \"MLP Neural Net (100)\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and calculate precision\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = precision_score(Y_test, y_pred)\n",
    "    results[name] = precision\n",
    "\n",
    "df_results = pd.DataFrame(results.items(), columns=[\"Model\", \"Precision\"])\n",
    "df_results = df_results.sort_values(by=\"Precision\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_results)\n",
    "\n",
    "best_model = df_results.iloc[0]\n",
    "print(f\"\\nBest model: {best_model['Model']} với Precision = {best_model['Precision']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
