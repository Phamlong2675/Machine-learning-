{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328347d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/klinhfhm/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/klinhfhm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "fc40507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "05262450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "985cf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english') \n",
    "    \n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and \n",
    "            word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "    return tweets_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "0a081abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    # start with an empty dict and populate it by looping over all tweets\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs\n",
    "\n",
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "fd9facc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        if (word, 1) in freqs.keys():\n",
    "            x[0,1] += freqs[(word, 1)]\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        if (word, 0) in freqs.keys():\n",
    "            x[0,2] += freqs[(word, 0)]\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "b1610daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "944f4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logistic(x, y, theta, alpha, num_iters):\n",
    "    m = len(x)\n",
    "    losses = []\n",
    "    for i in range(num_iters):\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # avoid log(0) → clip h\n",
    "        h = np.clip(h, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        J = - (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h))) / m \n",
    "        losses.append(J.item())   # store as clean float\n",
    "        \n",
    "        theta = theta - (alpha * np.dot(x.T, (h-y))) / m\n",
    "    return losses, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "4c911b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred\n",
    "def predict_logistic(X, w):\n",
    "    z = np.dot(X, w)\n",
    "    h = sigmoid(z)\n",
    "    return (h >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "2f416e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "X_test = np.zeros((len(test_x), 3))\n",
    "for i in range(len(test_x)):\n",
    "    X_test[i, :] = extract_features(test_x[i], freqs)\n",
    "Y_test = np.squeeze(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e501f5",
   "metadata": {},
   "source": [
    "**TASK 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "17625017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline (No Scaler)\n",
      "Final Loss: 0.0964\n",
      "Accuracy: 0.9960\n",
      "Precision: 0.9930\n",
      "\n",
      "MinMax\n",
      "Final Loss: 0.6931\n",
      "Accuracy: 0.9825\n",
      "Precision: 0.9979\n",
      "\n",
      "Standard\n",
      "Final Loss: 0.6931\n",
      "Accuracy: 0.9695\n",
      "Precision: 0.9425\n",
      "\n",
      "Robust\n",
      "Final Loss: 0.6931\n",
      "Accuracy: 0.9935\n",
      "Precision: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "scalers = {\n",
    "    \"Baseline (No Scaler)\": None,\n",
    "    \"MinMax\": MinMaxScaler(),\n",
    "    \"Standard\": StandardScaler(),\n",
    "    \"Robust\": RobustScaler()\n",
    "}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    \n",
    "    if scaler:\n",
    "        X_train_s = np.hstack([np.ones((X.shape[0], 1)), scaler.fit_transform(X[:, 1:])])\n",
    "        X_test_s  = np.hstack([np.ones((X_test.shape[0], 1)), scaler.transform(X_test[:, 1:])])\n",
    "    else:\n",
    "        X_train_s, X_test_s = X, X_test\n",
    "    \n",
    "    w_init = np.zeros((X_train_s.shape[1], 1))\n",
    "    losses, w = gradient_descent_logistic(X_train_s, Y, w_init, alpha=1e-9, num_iters=20000)\n",
    "    \n",
    "    y_pred = predict_logistic(X_test_s, w)\n",
    "    print(f\"Final Loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(Y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(Y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b98a7",
   "metadata": {},
   "source": [
    "**No Scaler:** Accuracy = 0.9960 → This is the highest score, showing that raw word frequency features are already well scaled for logistic regression. The model can separate positive vs. negative tweets effectively without normalization.\n",
    "\n",
    "**Min-Max Scaler:** Accuracy = 0.9825 → Accuracy drops compared to the baseline. By compressing all feature values into the [0,1] range, MinMax scaling reduces the separation between tweets with many emotional words and those with fewer. This weakens the discriminative power of the decision boundary.\n",
    "\n",
    "**Standard Scaler:** Accuracy = 0.9695 → Performance decreases further. Since word count features are not normally distributed, standardization distorts their distribution and reduces model performance.\n",
    "\n",
    "**Robust Scaler:** Accuracy = 0.9935 → Very close to the baseline. Because the dataset does not contain significant outliers in word frequencies, median/IQR-based scaling offers little extra benefit but also does not harm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84699f",
   "metadata": {},
   "source": [
    "**TASK 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9db5ea",
   "metadata": {},
   "source": [
    "**Test with 2 features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "4893b590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision\n",
      "0        Random Forest    0.9925   0.992993\n",
      "1        Decision Tree    0.9895   0.990973\n",
      "2                  SVM    0.9930   0.988119\n",
      "3  Logistic Regression    0.9930   0.987154\n",
      "4          Naive Bayes    0.5050   0.857143\n",
      "\n",
      "Best model: Random Forest → Precision = 0.992992992992993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": LinearSVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Naive Bayes\": BernoulliNB()\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # scale features for models sensitive to magnitude\n",
    "    if name in [\"Logistic Regression\", \"SVM\", \"KNN\", \"MLP Neural Net\"]:\n",
    "        clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=False)),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "    else:\n",
    "        clf = model\n",
    "\n",
    "    clf.fit(X, Y)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    prec = precision_score(Y_test, y_pred, zero_division=0)\n",
    "\n",
    "    results.append([name, acc, prec])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\"])\n",
    "df_results = df_results.sort_values(by=\"Precision\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_results)\n",
    "print(\"\\nBest model:\", df_results.iloc[0][\"Model\"], \"→ Precision =\", df_results.iloc[0][\"Precision\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06941d9",
   "metadata": {},
   "source": [
    "**Test with 6 features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "b3ed45bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9940, Precision = 0.9901, Recall = 0.9980, F1 = 0.9940, ROC-AUC = 0.9940, Time = 0.0099 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "import math\n",
    "import time\n",
    "\n",
    "def extract_6features(tweet, freqs, process_tweet=process_tweet):\n",
    "    x = np.zeros((1, 7))\n",
    "    x[0,0] = 1\n",
    "\n",
    "    pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\",\n",
    "            \"we\", \"us\", \"our\", \"ours\", \"ourselves\",\n",
    "            \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"}\n",
    "\n",
    "    word_1 = process_tweet(tweet)\n",
    "    \n",
    "    # x1, x2\n",
    "    for word in word_1:\n",
    "        if (word, 1) in freqs.keys():\n",
    "            x[0,1] += freqs[(word, 1)]\n",
    "        \n",
    "        if (word, 0) in freqs.keys():\n",
    "            x[0,2] += freqs[(word, 0)]\n",
    "    \n",
    "    # x3\n",
    "    if re.search(r\"\\bno\\b\", tweet.lower()):\n",
    "        x[0,3] = 1\n",
    "    \n",
    "    # x4\n",
    "    tokens_raw = re.findall(r\"\\w+\", tweet.lower())\n",
    "    x[0,4] = sum(1 for t in tokens_raw if t in pronouns)\n",
    "    \n",
    "    # x5\n",
    "    if \"!\" in tweet:\n",
    "        x[0,5] = 1\n",
    "    \n",
    "    # x6\n",
    "    word_count = len(tokens_raw)\n",
    "    if word_count > 0:\n",
    "        x[0,6] = math.log(word_count)\n",
    "    \n",
    "    assert(x.shape == (1, 7))\n",
    "    return x\n",
    "X_6features = np.zeros((len(train_x), 7))\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    X_6features[i, :]= extract_6features(train_x[i], freqs)\n",
    "Y_6features = np.squeeze(train_y)\n",
    "X_test_6features = np.zeros((len(test_x), 7))\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    X_test_6features[i, :] = extract_6features(test_x[i], freqs)\n",
    "Y_test_6features = np.squeeze(test_y)\n",
    "start_time = time.time()\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_6features, Y_6features)\n",
    "y_pred = clf.predict(X_test_6features)\n",
    "time = time.time() - start_time\n",
    "accuracy = accuracy_score(Y_test_6features, y_pred)\n",
    "precision = precision_score(Y_test_6features, y_pred)\n",
    "recall = recall_score(Y_test_6features, y_pred)\n",
    "f1 = f1_score(Y_test_6features, y_pred)\n",
    "roc_auc = roc_auc_score(Y_test_6features, y_pred)\n",
    "\n",
    "print(f\"Accuracy = {accuracy:.4f}, \"\n",
    "      f\"Precision = {precision:.4f}, \"\n",
    "      f\"Recall = {recall:.4f}, \"\n",
    "      f\"F1 = {f1:.4f}, \"\n",
    "      f\"ROC-AUC = {roc_auc:.4f}, \"\n",
    "      f\"Time = {time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "989d2dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision\n",
      "0        Decision Tree    0.9910   0.992972\n",
      "1        Random Forest    0.9920   0.990040\n",
      "2  Logistic Regression    0.9940   0.988142\n",
      "3                  SVM    0.9930   0.987154\n",
      "4          Naive Bayes    0.5765   0.723032\n",
      "\n",
      "Best model: Decision Tree → Precision = 0.9929718875502008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# List of models to test\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": BernoulliNB(),\n",
    "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Scale only for models sensitive to magnitude\n",
    "    if name in [\"Logistic Regression\", \"SVM\"]:\n",
    "        clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=False)),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "    else:\n",
    "        clf = model\n",
    "\n",
    "    clf.fit(X_6features, Y_6features)\n",
    "    y_pred = clf.predict(X_test_6features)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    accuracy = accuracy_score(Y_test_6features, y_pred)\n",
    "    precision = precision_score(Y_test_6features, y_pred, zero_division=0)\n",
    "    \n",
    "    results.append([name, accuracy, precision])\n",
    "\n",
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\"])\n",
    "df_results = df_results.sort_values(by=\"Precision\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_results)\n",
    "print(\"\\nBest model:\", df_results.iloc[0][\"Model\"], \"→ Precision =\", df_results.iloc[0][\"Precision\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
