{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328347d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a16717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "985cf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True) #the tokenizer will downcase everything except for emoticons\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and   # remove stopwords\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a081abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    # start with an empty dict and populate it by looping over all tweets\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs\n",
    "\n",
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9facc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        if (word, 1) in freqs.keys():\n",
    "            x[0,1] += freqs[(word, 1)]\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        if (word, 0) in freqs.keys():\n",
    "            x[0,2] += freqs[(word, 0)]\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1610daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    h = 1. / (1. + np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb6cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logistic(x, y, theta, alpha, num_iters):\n",
    "    # get 'm', the number of rows in matrix X\n",
    "    m = len(x)\n",
    "    losses = []\n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = - (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h))) / float(m)\n",
    "        losses.append(float(J))\n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha * np.dot(x.T, (h-y))) / float(m)\n",
    "    \n",
    "    J = float(J)\n",
    "\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c911b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86b971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic(X, w):\n",
    "    z = np.dot(X, w)\n",
    "    h = sigmoid(z)\n",
    "    return (h >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ed777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "X_test = np.zeros((len(test_x), 3))\n",
    "for i in range(len(test_x)):\n",
    "    X_test[i, :] = extract_features(test_x[i], freqs)\n",
    "Y_test = np.squeeze(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc38a5d",
   "metadata": {},
   "source": [
    "***Task 1***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de89be3",
   "metadata": {},
   "source": [
    "***Custom Logistic Regression:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa320eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_2228\\3902507023.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  losses.append(float(J))\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_2228\\3902507023.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  J = float(J)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "J, w = gradient_descent_logistic(X, Y, np.zeros((3, 1)), 1e-9, 10000)\n",
    "y_pred_custom = predict_logistic(X_test, w)\n",
    "\n",
    "time_custom = time.time() - start_time\n",
    "\n",
    "accuracy_custom = accuracy_score(test_y, y_pred_custom)\n",
    "precision_custom = precision_score(test_y, y_pred_custom)\n",
    "recall_custom = recall_score(Y_test, y_pred_custom)\n",
    "f1_custom = f1_score(Y_test, y_pred_custom)\n",
    "roc_auc_custom = roc_auc_score(test_y, y_pred_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2198edc",
   "metadata": {},
   "source": [
    "***Sklearn Logistic Regression:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30db0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.squeeze(train_y)\n",
    "start_time = time.time()\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "y_pred_sklearn = clf.predict(X_test)\n",
    "\n",
    "time_sklearn = time.time() - start_time\n",
    "\n",
    "accuracy_sklearn = accuracy_score(Y_test, y_pred_sklearn)\n",
    "precision_sklearn = precision_score(Y_test, y_pred_sklearn)\n",
    "recall_sklearn = recall_score(Y_test, y_pred_sklearn)\n",
    "f1_sklearn = f1_score(Y_test, y_pred_sklearn)\n",
    "roc_auc_sklearn = roc_auc_score(Y_test, y_pred_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bc1d5",
   "metadata": {},
   "source": [
    "***Comparison:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44135561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom : Accuracy = 0.9960, Precision = 0.9930, Recall = 0.9990, F1 = 0.9960, ROC-AUC = 0.9960, Time = 4.8870 seconds\n",
      "Sklearn: Accuracy = 0.9950, Precision = 0.9920, Recall = 0.9980, F1 = 0.9950, ROC-AUC = 0.9960, Time = 0.0397 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Custom : Accuracy = {accuracy_custom:.4f}, \"\n",
    "      f\"Precision = {precision_custom:.4f}, \"\n",
    "      f\"Recall = {recall_custom:.4f}, \"\n",
    "      f\"F1 = {f1_custom:.4f}, \"\n",
    "      f\"ROC-AUC = {roc_auc_custom:.4f}, \"\n",
    "      f\"Time = {time_custom:.4f} seconds\")\n",
    "\n",
    "print(f\"Sklearn: Accuracy = {accuracy_sklearn:.4f}, \"\n",
    "      f\"Precision = {precision_sklearn:.4f}, \"\n",
    "      f\"Recall = {recall_sklearn:.4f}, \"\n",
    "      f\"F1 = {f1_sklearn:.4f}, \"\n",
    "      f\"ROC-AUC = {roc_auc_sklearn:.4f}, \"\n",
    "      f\"Time = {time_sklearn:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582affe",
   "metadata": {},
   "source": [
    "- Custom logistic regression model:\n",
    "This model does not apply regularization, so the weights are optimized more closely to the training data. As a result, the metrics such as Accuracy, Precision, Recall, and F1 are slightly higher compared to the library-based model. However, the drawback is a higher risk of overfitting when applied to unseen data, since the model tends to fit too tightly to the training set.\n",
    "\n",
    "- Scikit-learn logistic regression model:\n",
    "This model uses L2 regularization by default to prevent overfitting and improve the model’s generalization ability. Due to the presence of regularization, some metrics such as Accuracy, Precision, Recall, and F1 are slightly lower than those of the custom model. On the other hand, scikit-learn has a clear advantage in training speed and stability, especially when working with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81b689",
   "metadata": {},
   "source": [
    "***Task 6***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05931d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_6features(tweet, freqs, process_tweet=process_tweet):\n",
    "    x = np.zeros((1, 7))\n",
    "    x[0,0] = 1\n",
    "\n",
    "    pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\",\n",
    "            \"we\", \"us\", \"our\", \"ours\", \"ourselves\",\n",
    "            \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"}\n",
    "\n",
    "    word_1 = process_tweet(tweet)\n",
    "    \n",
    "    # x1, x2\n",
    "    for word in word_1:\n",
    "        if (word, 1) in freqs.keys():\n",
    "            x[0,1] += freqs[(word, 1)]\n",
    "        \n",
    "        if (word, 0) in freqs.keys():\n",
    "            x[0,2] += freqs[(word, 0)]\n",
    "    \n",
    "    # x3\n",
    "    if re.search(r\"\\bno\\b\", tweet.lower()):\n",
    "        x[0,3] = 1\n",
    "    \n",
    "    # x4\n",
    "    tokens_raw = re.findall(r\"\\w+\", tweet.lower())\n",
    "    x[0,4] = sum(1 for t in tokens_raw if t in pronouns)\n",
    "    \n",
    "    # x5\n",
    "    if \"!\" in tweet:\n",
    "        x[0,5] = 1\n",
    "    \n",
    "    # x6\n",
    "    word_count = len(tokens_raw)\n",
    "    if word_count > 0:\n",
    "        x[0,6] = math.log(word_count)\n",
    "    \n",
    "    assert(x.shape == (1, 7))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e39149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_6features = np.zeros((len(train_x), 7))\n",
    "for i in range(len(train_x)):\n",
    "    X_6features[i, :]= extract_6features(train_x[i], freqs)\n",
    "Y_6features = np.squeeze(train_y)\n",
    "\n",
    "X_test_6features = np.zeros((len(test_x), 7))\n",
    "for i in range(len(test_x)):\n",
    "    X_test_6features[i, :] = extract_6features(test_x[i], freqs)\n",
    "Y_test_6features = np.squeeze(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa716fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_6features, Y_6features)\n",
    "y_pred = clf.predict(X_test_6features)\n",
    "time = time.time() - start_time\n",
    "accuracy = accuracy_score(Y_test_6features, y_pred)\n",
    "precision = precision_score(Y_test_6features, y_pred)\n",
    "recall = recall_score(Y_test_6features, y_pred)\n",
    "f1 = f1_score(Y_test_6features, y_pred)\n",
    "roc_auc = roc_auc_score(Y_test_6features, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f217ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9940, Precision = 0.9911, Recall = 0.9970, F1 = 0.9940, ROC-AUC = 0.9940, Time = 0.0628 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {accuracy:.4f}, \"\n",
    "      f\"Precision = {precision:.4f}, \"\n",
    "      f\"Recall = {recall:.4f}, \"\n",
    "      f\"F1 = {f1:.4f}, \"\n",
    "      f\"ROC-AUC = {roc_auc:.4f}, \"\n",
    "      f\"Time = {time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf5852",
   "metadata": {},
   "source": [
    "- With two features, the model achieved:\n",
    "Accuracy = 0.9950, Precision = 0.9920, Recall = 0.9980, F1 = 0.9950, and ROC-AUC = 0.9960. \n",
    "- When four additional features were introduced, the performance slightly dropped to: \n",
    "Accuracy = 0.9940, Precision = 0.9911, Recall = 0.9970, F1 = 0.9940, and ROC-AUC = 0.9940. \n",
    "- This decrease occurred because the extra features added complexity but contributed little useful information, introducing noise. Moreover, scikit-learn’s default L2 regularization penalized unnecessary weights, which reduced overfitting but also led to slightly lower precision and recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
